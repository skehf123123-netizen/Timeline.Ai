import streamlit as st
import os
import time
import pandas as pd
import base64
import json
import io
import difflib
import tempfile
import shutil
import concurrent.futures
import math
import random
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI, RateLimitError, APIConnectionError, BadRequestError
from supabase import create_client, Client
from PIL import Image, ImageOps

# =====================================
# 0) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ˆì™¸ ì²˜ë¦¬ (Dependency Check)
# =====================================
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ë„ ì•±ì´ í„°ì§€ì§€ ì•Šë„ë¡ í”Œë˜ê·¸ ì„¤ì • ë° ì„í¬íŠ¸ ì²˜ë¦¬

# 1. HEIF ì´ë¯¸ì§€ ì§€ì›
try:
    from pillow_heif import register_heif_opener
    register_heif_opener()
except ImportError:
    pass  # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìœ¼ë©´ HEIC ì§€ì› ì•ˆí•¨

# 2. DecompressionBomb ë°©ì–´ (ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ ì²˜ë¦¬)
Image.MAX_IMAGE_PIXELS = 100_000_000

# 3. openpyxl (ì—‘ì…€ ì €ì¥)
try:
    import openpyxl
    HAS_OPENPYXL = True
except ImportError:
    HAS_OPENPYXL = False

# 4. pypdf (PDF ë¶„ì„)
try:
    from pypdf import PdfReader
    HAS_PYPDF = True
except ImportError:
    HAS_PYPDF = False

# 5. pydub (ì˜¤ë””ì˜¤ ë³€í™˜)
try:
    from pydub import AudioSegment
    HAS_PYDUB = True
except ImportError:
    HAS_PYDUB = False

# 6. moviepy (ë¹„ë””ì˜¤ ë¶„ì„)
try:
    import moviepy.editor as mp
    HAS_MOVIEPY = True
except (ImportError, RuntimeError, OSError):
    HAS_MOVIEPY = False

# Pillow Resampling í˜¸í™˜ì„± (êµ¬ë²„ì „ ëŒ€ì‘)
try:
    RESAMPLING_METHOD = Image.Resampling.LANCZOS
except AttributeError:
    RESAMPLING_METHOD = Image.LANCZOS

# =====================================
# í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
# =====================================
st.set_page_config(
    page_title="Timeline.Ai", 
    page_icon="âš–ï¸", 
    layout="wide", 
    initial_sidebar_state="expanded"
)

load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
REDIRECT_URL = os.getenv("SUPABASE_REDIRECT_URL", "http://localhost:8501")

if not SUPABASE_URL or not SUPABASE_KEY:
    st.error("âŒ .env íŒŒì¼ì—ì„œ Supabase ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

if not OPENAI_KEY:
    st.error("âŒ .env íŒŒì¼ì—ì„œ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
client = OpenAI(api_key=OPENAI_KEY)

if "auth_mode" not in st.session_state:
    st.session_state["auth_mode"] = "login"
if "user" not in st.session_state:
    st.session_state["user"] = None
if 'is_dark_mode' not in st.session_state:
    st.session_state['is_dark_mode'] = False
if 'result_data' not in st.session_state:
    st.session_state.result_data = []

# =====================================
# 1) ìƒìˆ˜ ì„¤ì •
# =====================================
MAX_MEDIA_MS = 2 * 60 * 60 * 1000  # 2ì‹œê°„
MAX_PDF_CHUNK_SIZE = 15000         # PDF ì²­í¬ í¬ê¸°

MAX_IMAGES_PRO = 100
MAX_IMAGE_DIMENSION = 3072 # ì´ë¯¸ì§€ ë¶„ì„ ì‹œ ë¦¬ì‚¬ì´ì¦ˆ ì œí•œ (ë¹„ìš© ì ˆê° ë° ì†ë„)
DEFAULT_BATCH_SIZE = 3     # ë°°ì¹˜ ì‚¬ì´ì¦ˆ
MAX_ZIP_SIZE_MB = 200      # ZIP ë‹¤ìš´ë¡œë“œ ìš©ëŸ‰ ì œí•œ
DEFAULT_MAX_TOKENS = 2048  # [ìˆ˜ì •] ê¸°ë³¸ í† í° ìˆ˜ ì œí•œ í•˜í–¥

# =========================================================
# [ìˆ˜ì • 1/3] ìŠ¤í‚¤ë§ˆ: ë‚ ì§œ/ì‹œê°„/íƒ€ì„ìŠ¤íƒ¬í”„ "ë¯¸í™•ì¸" í—ˆìš©
# =========================================================
# =========================================================
# [ìˆ˜ì •] ìŠ¤í‚¤ë§ˆ: Pattern(ì •ê·œì‹) ì œê±° -> AIê°€ ììœ ë¡­ê²Œ ì¶”ì¶œ í›„ í›„ì²˜ë¦¬
# =========================================================
TIMELINE_SCHEMA = {
    "type": "json_schema",
    "json_schema": {
        "name": "timeline_response",
        "strict": False,
        "schema": {
            "type": "object",
            "properties": {
                "messages": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            # pattern ì •ê·œì‹ ì œì•½ ì œê±° (AIê°€ ììœ ë¡­ê²Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê°€ëŠ¥í•˜ê²Œ í•¨)
                            "timestamp": {"type": "string"}, 
                            "date": {"type": "string"},
                            "time": {"type": "string"},
                            "context": {"type": "string"},
                            "sender": {"type": "string"},
                            "content": {"type": "string"},
                            "importance": {"type": "string", "enum": ["ìƒ", "ì¤‘", "í•˜", "ë¯¸ìƒ"]},
                            "is_estimated": {"type": "boolean"}
                        },
                        "required": ["timestamp", "date", "time", "context", "sender", "content", "importance", "is_estimated"],
                        "additionalProperties": False
                    }
                }
            },
            "required": ["messages"],
            "additionalProperties": False
        }
    }
}

# =========================================================
# [ìˆ˜ì • 1/3] ê³µí†µ ì¸ìŠ¤íŠ¸ëŸ­ì…˜: 1970 ê°•ì œ ê¸ˆì§€, ë¯¸í™•ì¸ ì‚¬ìš©
# =========================================================
COMMON_SCHEMA_INSTRUCTION = """
    [í•„ìˆ˜ í¬í•¨ í•„ë“œ (Strict Schema)]
    ëª¨ë“  ë©”ì‹œì§€ ê°ì²´ëŠ” ì•„ë˜ í•„ë“œë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ë¯¸í™•ì¸'ìœ¼ë¡œ ë‘ì‹­ì‹œì˜¤.
    - timestamp: "YYYY-MM-DD HH:MM:SS" ë˜ëŠ” "ë¯¸í™•ì¸"
    - date: "YYYY-MM-DD" ë˜ëŠ” "ë¯¸í™•ì¸"
    - time: "HH:MM:SS" ë˜ëŠ” "ë¯¸í™•ì¸"
    - context: ëŒ€í™”ë°© ì´ë¦„, ë¬¸ì„œ ì œëª©, ìƒí™© ì„¤ëª… ë“± (ì‹ë³„ ë¶ˆê°€ ì‹œ 'ë¯¸í™•ì¸')
    - sender: ë°œí™”ì ì´ë¦„ (ì‹ë³„ ë¶ˆê°€ ì‹œ 'ë¶ˆìƒ')
    - content: ë‚´ìš© ì›ë¬¸ (Verbatim)
    - importance: "ìƒ/ì¤‘/í•˜/ë¯¸ìƒ"
    - is_estimated: boolean (true/false)  â€» ë‚ ì§œ/ì‹œê°„ì´ ë¯¸í™•ì¸ ë˜ëŠ” ì¶”ì •ì´ë©´ true
"""

# =====================================
# 2) ìœ í‹¸ë¦¬í‹° ë° AI ë¶„ì„ í•¨ìˆ˜
# =====================================
def safe_json_loads(s):
    try:
        return json.loads(s)
    except Exception:
        try:
            if not isinstance(s, str):
                s = str(s)
            start = s.find("{")
            end = s.rfind("}")
            if start != -1 and end != -1 and end > start:
                return json.loads(s[start:end + 1])
        except Exception:
            pass
        return {}

def optimize_image_bytes(image_bytes: bytes):
    try:
        with Image.open(io.BytesIO(image_bytes)) as img:
            img = ImageOps.exif_transpose(img)
            if img.mode in ("RGBA", "P"):
                img = img.convert("RGB")

            w, h = img.size
            
            # [ìˆ˜ì •] ê¸´ ìŠ¤í¬ë¦°ìƒ· ëŒ€ì‘ ë¡œì§: 
            # ì„¸ë¡œ(h)ê°€ ì•„ë¬´ë¦¬ ê¸¸ì–´ë„, ê°€ë¡œ(w)ê°€ 1024px ì´í•˜ë¼ë©´ ë¦¬ì‚¬ì´ì¦ˆ í•˜ì§€ ì•ŠìŒ (í™”ì§ˆ ìœ ì§€)
            # ê°€ë¡œê°€ ë„ˆë¬´ í´ ë•Œë§Œ ì¤„ì—¬ì„œ AI í† í° ë¹„ìš© ì ˆì•½
            
            # ê¸°ì¤€: ê°€ë¡œê°€ 2048ë³´ë‹¤ í¬ë©´ ì¤„ì„, ì•„ë‹ˆë©´ ì›ë³¸ ìœ ì§€
            if w > 2048:
                scale = 2048 / w
                new_w = int(w * scale)
                new_h = int(h * scale)
                img = img.resize((new_w, new_h), RESAMPLING_METHOD)
            
            # (ì˜µì…˜) í•˜ì§€ë§Œ ë†’ì´ê°€ OpenAI ì œí•œ(ì•½ 10,000~15,000px)ì„ ë„˜ì–´ê°€ë©´ ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
            # ê·¹ë‹¨ì ìœ¼ë¡œ ê¸´ ì´ë¯¸ì§€ëŠ” ë°˜ìœ¼ë¡œ ìë¥´ëŠ” ë“±ì˜ ì²˜ë¦¬ê°€ í•„ìš”í•˜ì§€ë§Œ,
            # ìš°ì„ ì€ ë†’ì´ ì œí•œì„ ë„‰ë„‰í•˜ê²Œ 8000ìœ¼ë¡œ ë‘ 
            elif h > 8000:
                # ê°€ë¡œí­ì´ ì¶©ë¶„í•˜ë‹¤ë©´ ë†’ì´ë§Œ ì¤„ì´ëŠ” ê±´ ë¹„ìœ¨ ê¹¨ì§ -> ë¹„ìœ¨ ìœ ì§€í•˜ë©° ì¤„ì„
                scale = 8000 / h
                # ë‹¨, ì´ë ‡ê²Œ ì¤„ì˜€ì„ ë•Œ ê°€ë¡œê°€ ë„ˆë¬´ ì‘ì•„ì§€ë©´(600px ë¯¸ë§Œ) ì•ˆ ì¤„ì„
                if (w * scale) > 600:
                    new_w = int(w * scale)
                    new_h = int(h * scale)
                    img = img.resize((new_w, new_h), RESAMPLING_METHOD)

            buffer = io.BytesIO()
            # í…ìŠ¤íŠ¸ ì„ ëª…ë„ë¥¼ ìœ„í•´ í’ˆì§ˆ 100 ì„¤ì •
            img.save(buffer, format="JPEG", quality=85)
            return base64.b64encode(buffer.getvalue()).decode("utf-8")

    except Image.DecompressionBombError:
        print("Image too large (DecompressionBomb)")
        return None
    except Exception as e:
        print(f"Optimize Error: {e}")
        return None

# =========================================================
# [ìˆ˜ì • 1/3] normalize_message_item: 1970 ê°•ì œ ì„¸íŒ… ì œê±°, ë¯¸í™•ì¸ ìœ ì§€
# =========================================================
def normalize_message_item(item: dict) -> dict:
    """
    - timestamp/date/timeì´ ì •ìƒ í¬ë§·ì´ë©´ date/time ë³´ì •
    - ì¸ì‹ ë¶ˆê°€/ëˆ„ë½ì´ë©´ ë¯¸í™•ì¸ ìœ ì§€ + is_estimated=True ê°•ì œ
    """
    ts_str = (item.get("timestamp") or "").strip()

    # ê¸°ë³¸ê°’ ì•ˆì „í™”
    if not item.get("timestamp"):
        item["timestamp"] = "ë¯¸í™•ì¸"
    if not item.get("date"):
        item["date"] = "ë¯¸í™•ì¸"
    if not item.get("time"):
        item["time"] = "ë¯¸í™•ì¸"
    if "is_estimated" not in item:
        item["is_estimated"] = True
    if not item.get("importance"):
        item["importance"] = "ë¯¸ìƒ"

    # timestampê°€ ì •ìƒ í¬ë§·ì´ë©´ date/time ë™ê¸°í™”
    if ts_str and ts_str != "ë¯¸í™•ì¸":
        try:
            dt = pd.to_datetime(ts_str, errors="raise")
            item["timestamp"] = dt.strftime("%Y-%m-%d %H:%M:%S")
            item["date"] = dt.strftime("%Y-%m-%d")
            item["time"] = dt.strftime("%H:%M:%S")
        except Exception:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¯¸í™•ì¸ ì²˜ë¦¬
            item["timestamp"] = "ë¯¸í™•ì¸"
            item["date"] = "ë¯¸í™•ì¸"
            item["time"] = "ë¯¸í™•ì¸"
            item["is_estimated"] = True

    # date/timeì´ ë¯¸í™•ì¸ì´ë©´ is_estimatedëŠ” trueê°€ ìì—°ìŠ¤ëŸ¬ì›€
    if item.get("timestamp") == "ë¯¸í™•ì¸" or item.get("date") == "ë¯¸í™•ì¸" or item.get("time") == "ë¯¸í™•ì¸":
        item["is_estimated"] = True

    return item

def call_chat_json_robust(api_key, messages, max_tokens=DEFAULT_MAX_TOKENS):
    """
    GPT í˜¸ì¶œ ì•ˆì „ ë˜í¼: 4o(Schema) -> 4o-mini(Schema) -> 4o(JSON) í´ë°± ì „ëµ
    [ìˆ˜ì •] max_tokens ê¸°ë³¸ê°’ì„ DEFAULT_MAX_TOKENS(2048)ë¡œ ë³€ê²½
    """
    local_client = OpenAI(api_key=api_key)
    
    strategies = [
        ("gpt-4o-2024-08-06", TIMELINE_SCHEMA),
        ("gpt-4o-mini", TIMELINE_SCHEMA),
        ("gpt-4o", {"type": "json_object"})
    ]

    last_error = None
    
    for model, resp_format in strategies:
        retries = 0
        while retries <= 2:
            try:
                response = local_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=0.0,
                    response_format=resp_format
                )
                content = response.choices[0].message.content
                data = safe_json_loads(content)
                if "messages" in data:
                    return data
                raise ValueError("JSON Key 'messages' not found")
            except (RateLimitError, APIConnectionError):
                retries += 1
                time.sleep(2 + random.random())
            except BadRequestError:
                # ìŠ¤í‚¤ë§ˆ ë¯¸ì§€ì› ë“±ì˜ ì´ìœ ë¡œ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ì „ëµìœ¼ë¡œ
                break
            except Exception as e:
                retries += 1
                last_error = e
                time.sleep(1)
    
    print(f"[API Failed] Last Error: {last_error}")
    return {"messages": []}

def transcribe_audio_chunk(file_path):
    last_error = None
    for attempt in range(3):
        try:
            with open(file_path, "rb") as f:
                return client.audio.transcriptions.create(
                    model="whisper-1",
                    file=f,
                    response_format="text",
                )
        except Exception as e:
            last_error = e
            time.sleep(1 * (attempt + 1))
    raise last_error

def calculate_similarity(s1, s2):
    if pd.isna(s1): s1 = ""
    if pd.isna(s2): s2 = ""
    return difflib.SequenceMatcher(None, str(s1), str(s2)).ratio() * 100

def normalize_date(d):
    if pd.isna(d): return ""
    return str(d).strip()[:10]

def evaluate_results(df_truth, df_ai):
    used_ai_indices = set()
    report_data = []
    total_score = 0.0
    matched_count = 0

    for i in range(len(df_truth)):
        truth_row = df_truth.iloc[i]
        best_idx = None
        best_sim = -1.0
        truth_content = truth_row.get("content", "")

        for idx, ai_row in df_ai.iterrows():
            if idx in used_ai_indices:
                continue
            sim = calculate_similarity(truth_content, ai_row.get("content", ""))
            if sim > best_sim:
                best_sim = sim
                best_idx = idx
       
        if best_idx is None or best_sim < 50.0:
            report_data.append({
                "ID": i+1, "ìƒíƒœ": "âŒ ë¯¸íƒì§€", "ì •ë‹µë‚´ìš©": truth_content, "AIì˜ˆì¸¡": "-", "ì ìˆ˜": 0
            })
            continue

        used_ai_indices.add(best_idx)
        matched_count += 1
        ai_row = df_ai.loc[best_idx]
       
        content_score = best_sim
        date_match = normalize_date(truth_row.get("date")) == normalize_date(ai_row.get("date"))
        date_score = 100 if date_match else 0
        imp_match = str(truth_row.get("importance")) == str(ai_row.get("importance"))
        imp_score = 100 if imp_match else 0
        sender_score = calculate_similarity(truth_row.get("sender"), ai_row.get("sender"))

        final_score = (content_score * 0.5) + (date_score * 0.2) + (imp_score * 0.2) + (sender_score * 0.1)
        total_score += final_score

        report_data.append({
            "ID": i+1,
            "ìƒíƒœ": "âœ… ë§¤ì¹­ë¨",
            "ì •ë‹µë‚´ìš©": truth_content,
            "AIì˜ˆì¸¡": ai_row.get("content"),
            "ë‚´ìš©ìœ ì‚¬ë„": round(content_score, 1),
            "ë‚ ì§œì¼ì¹˜": "O" if date_match else "X",
            "ì ìˆ˜": round(final_score, 1),
        })

    avg_score = total_score / matched_count if matched_count > 0 else 0
    return avg_score, pd.DataFrame(report_data)

def encode_image(image_file):
    image_file.seek(0)
    return base64.b64encode(image_file.read()).decode('utf-8')

# =========================================================
# ì•ˆì „í•œ ì„ì‹œ íŒŒì¼ ì²˜ë¦¬ (Dependency Check ì¶”ê°€)
# =========================================================
def extract_audio_from_video(video_file):
    if not HAS_MOVIEPY:
        st.error("âŒ 'moviepy' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì˜ìƒ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    tfile = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    tfile.write(video_file.read())
    tfile.close()
    
    video_path = tfile.name
    audio_path = None
    
    try:
        video = mp.VideoFileClip(video_path)
        if video.audio is None:
            st.warning(f"ğŸ”‡ '{video_file.name}' ì˜ìƒì— ì˜¤ë””ì˜¤ íŠ¸ë™ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        duration_ms = video.duration * 1000 if video.duration is not None else 0
        audio_clip = video.audio
        
        if duration_ms > MAX_MEDIA_MS:
            st.warning("ğŸ¬ ì˜ìƒì´ ë„ˆë¬´ ê¸¸ì–´ ì•ë¶€ë¶„ 2ì‹œê°„ë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")
            audio_clip = audio_clip.subclip(0, MAX_MEDIA_MS / 1000.0)
        
        afile = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        afile.close()
        audio_path = afile.name
        
        audio_clip.write_audiofile(audio_path, logger=None)
        return audio_path

    except Exception as e:
        st.error(f"ì˜ìƒ ë³€í™˜ ì˜¤ë¥˜: {e}")
        return None
    finally:
        try:
            if 'video' in locals(): video.close()
            if 'audio_clip' in locals() and audio_clip: audio_clip.close()
        except: pass
        if os.path.exists(video_path):
            os.remove(video_path)

def process_audio_file(file_obj_or_path):
    """
    [ìˆ˜ì •] í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ì´ ì•„ë‹ˆë¼, ì²­í¬ë³„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ë„ë¡ ë³€ê²½
    Returns: list[str]
    """
    if not HAS_PYDUB:
        st.error("âŒ 'pydub' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì˜¤ë””ì˜¤ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    if isinstance(file_obj_or_path, str):
        file_path = file_obj_or_path
        should_cleanup_input = False
    else:
        tfile = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        tfile.write(file_obj_or_path.read())
        tfile.close()
        file_path = tfile.name
        should_cleanup_input = True

    transcript_chunks = []
    try:
        sound = AudioSegment.from_file(file_path)
        if len(sound) > MAX_MEDIA_MS:
            sound = sound[:MAX_MEDIA_MS]
        
        # 10ë¶„ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
        chunk_length_ms = 10 * 60 * 1000
        chunks = [sound[i:i + chunk_length_ms] for i in range(0, len(sound), chunk_length_ms)]
        
        for i, chunk in enumerate(chunks):
            cfile = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
            cfile.close()
            chunk_path = cfile.name
            
            chunk.export(chunk_path, format="mp3")
            try:
                transcript = transcribe_audio_chunk(chunk_path)
                if transcript and transcript.strip():
                    transcript_chunks.append(transcript)
            finally:
                if os.path.exists(chunk_path): os.remove(chunk_path)
                
    except Exception as e:
        st.error(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    finally:
        if should_cleanup_input and os.path.exists(file_path):
            os.remove(file_path)
            
    return transcript_chunks

# =========================================================
# AI ë¶„ì„ í”„ë¡¬í”„íŠ¸ & ì›Œì»¤
# =========================================================
# =========================================================
# [ìˆ˜ì • 2/3] ë³‘ë ¬ ìˆœì„œ ìœ ì§€: batch_start_indexë¥¼ ì›Œì»¤ì— ì „ë‹¬
# =========================================================
def analyze_image_batch_worker(batch_data, api_key, batch_start_index: int):
    system_prompt = f"""
    ë‹¹ì‹ ì€ ë²•ì›ì— ì œì¶œí•  ì¦ê±°ìë£Œë¥¼ ë¶„ì„í•˜ëŠ” 'ë””ì§€í„¸ í¬ë Œì‹ ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
    ì´ ë‚´ìš©ì€ SNS ëŒ€í™”ê¸°ë¡ì…ë‹ˆë‹¤.
    
    [í•µì‹¬ ì›ì¹™]
    1. ì›ë¬¸ ìœ ì§€: ëŒ€í™” ë‚´ìš©ì„ ë¹ ì§ì—†ì´ ì „ì‚¬í•˜ì‹­ì‹œì˜¤. (ë‚ ì§œ/ì‹œê°„ í‘œì‹œ í¬í•¨) ì˜¤íƒ€, ë¹„ì†ì–´, ì´ëª¨í‹°ì½˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ì „ì‚¬í•˜ì‹­ì‹œì˜¤.
    2. ê°ê´€ì„±: ì¶”ì¸¡ì„± ë‚´ìš©ì€ ë°°ì œí•˜ê³  "[íŒë…ë¶ˆê°€]"ë¡œ í‘œê¸°í•˜ì‹­ì‹œì˜¤.
    3. ì‹œê°„ ì •ë³´: ì´ë¯¸ì§€ ë‚´ ì‹œê°„ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í•˜ë˜, ì—†ê±°ë‚˜ ë¶ˆëª…í™•í•˜ë©´ "ë¯¸í™•ì¸"ìœ¼ë¡œ í‘œê¸°í•˜ê³  is_estimated=trueë¡œ í‘œì‹œí•˜ì‹­ì‹œì˜¤.
    4. í™”ë©´ ìƒë‹¨ì´ë‚˜ ì¤‘ê°„ì— ìˆëŠ” "202xë…„ xì›” xì¼" ê°™ì€ ë‚ ì§œ ì •ë³´ë¥¼ ë†“ì¹˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    3. ì‹œê°„(ì˜¤ì „/ì˜¤í›„) ì •ë³´ê°€ ë³´ì´ë©´ 24ì‹œê°„ì œë¡œ ë³€í™˜í•˜ì—¬ timestampì— ê¸°ë¡í•˜ì‹­ì‹œì˜¤.
    4. ë°œì‹ ì(sender) ì´ë¦„ì´ ì—†ìœ¼ë©´ ë§í’ì„  ìœ„ì¹˜(ë…¸ë€ìƒ‰: ë‚˜, í°ìƒ‰: ìƒëŒ€ë°©)ë¥¼ ë³´ê³  íŒë‹¨í•˜ì—¬ 'ë‚˜' ë˜ëŠ” 'ìƒëŒ€ë°©'ìœ¼ë¡œ ì ìœ¼ì‹­ì‹œì˜¤.
    {COMMON_SCHEMA_INSTRUCTION}
    """
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": [{"type": "text", "text": "ì´ë¯¸ì§€ë“¤ì„ ë¶„ì„í•˜ì—¬ ì—…ë¡œë“œëœ ìˆœì„œë¥¼ í•´ì¹˜ì§€ ì•Šë„ë¡ JSONìœ¼ë¡œ ë°˜í™˜í•˜ë¼. ë‚ ì§œ/ì‹œê°„ì´ ì—†ìœ¼ë©´ ë¯¸í™•ì¸ìœ¼ë¡œ ë‘”ë‹¤."}]}
    ]

    valid_files = []
    for fname, fbytes in batch_data:
        b64 = optimize_image_bytes(fbytes)
        if b64:
            messages[1]["content"].append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{b64}", "detail": "high"}
            })
            valid_files.append(fname)

    if not valid_files:
        return [], [x[0] for x in batch_data]

    try:
        res = call_chat_json_robust(api_key, messages)
        items = []
        for j, item in enumerate(res.get("messages", [])):
            item = normalize_message_item(item)
            item['source'] = 'ìŠ¤í¬ë¦°ìƒ·'
            item['filename'] = f"Batch_Start_{valid_files[0]}"
            if not item.get('context'): item['context'] = "ë©”ì‹ ì € ëŒ€í™”"

            # ì—…ë¡œë“œ ìˆœì„œ ê¸°ë°˜ ì •ë ¬ì„ ìœ„í•œ í‚¤(ë³‘ë ¬ ì„ì„ ë°©ì§€)
            # ë°°ì¹˜ ì‹œì‘ ì¸ë±ìŠ¤ + ë°°ì¹˜ ë‚´ë¶€ ë©”ì‹œì§€ ìˆœë²ˆ(ì†Œìˆ˜ì )ìœ¼ë¡œ ì•ˆì • ì •ë ¬
            item['upload_index'] = float(batch_start_index) + (j / 1000.0)

            items.append(item)
        return items, []
    except Exception as e:
        print(f"Batch Worker Error: {e}")
        return [], [x[0] for x in batch_data]

def analyze_pdf_chunk(text_chunk, page_info):
    prompt = f"""
    ë²•ì› ì œì¶œìš© ì¦ê±° ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ 'ì…ì¦ ì‚¬ì‹¤'ì„ JSONìœ¼ë¡œ ì¶”ì¶œí•˜ë¼.
    í˜„ì¬ ë¶„ì„ ì¤‘ì¸ ë¶€ë¶„: {page_info}
    
    [ê·œì¹™]
    1. ë¬¸ì„œì— ëª…ì‹œëœ ë‚ ì§œì™€ ì‚¬ê±´ì„ ì •í™•íˆ ë§¤ì¹­í•˜ë¼.
    2. í•µì‹¬ ë¬¸ì¥ì„ ìš”ì•½ ì—†ì´ ë°œì·Œí•˜ë¼.
    3. ë‚ ì§œ/ì‹œê°„ì´ ë¶ˆëª…í™•í•˜ë©´ "ë¯¸í™•ì¸"ìœ¼ë¡œ ë‘ê³  is_estimated=trueë¡œ í•˜ë¼.
    
    {COMMON_SCHEMA_INSTRUCTION}
    
    [ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¼ë¶€]
    {text_chunk}
    """
    return call_chat_json_robust(OPENAI_KEY, [{"role": "user", "content": prompt}])

def analyze_transcript_with_gpt(transcript_text, chunk_info=""):
    """
    [ìˆ˜ì •] chunk_infoë¥¼ ì¸ìë¡œ ë°›ì•„ í”„ë¡¬í”„íŠ¸ì— ë°˜ì˜
    """
    prompt = f"""
    ë²•ì› ì œì¶œìš© ë…¹ì·¨ë¡ì„ ì‘ì„±í•˜ë¼.
    ë¶„ì„ êµ¬ê°„: {chunk_info}
    
    [ê·œì¹™]
    1. ë°œí™” ë‚´ìš©ì€ ìš”ì•½í•˜ì§€ ë§ê³  ë¹„ì†ì–´, ì¶”ì„ìƒˆë¥¼ í¬í•¨í•˜ì—¬ ê·¸ëŒ€ë¡œ ì „ì‚¬í•˜ë¼.
    2. í™”ìê°€ ë¶ˆë¶„ëª…í•  ê²½ìš° 'í™”ìë¯¸ìƒ'ìœ¼ë¡œ í‘œê¸°í•˜ë¼.
    3. ë‚ ì§œ/ì‹œê°„ì´ ë¶ˆëª…í™•í•˜ë©´ "ë¯¸í™•ì¸"ìœ¼ë¡œ ë‘ê³  is_estimated=trueë¡œ í•˜ë¼.
    
    {COMMON_SCHEMA_INSTRUCTION}
    
    [ë…¹ì·¨ë¡ í…ìŠ¤íŠ¸]
    {transcript_text}
    """
    return call_chat_json_robust(OPENAI_KEY, [{"role": "user", "content": prompt}])

# =====================================
# í†µí•© ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜ (Dependency Check & ë³‘ë ¬ ì²˜ë¦¬)
# =====================================
def run_analysis(imgs, audio, video, pdf, plan_type="pro"):
    final_data = []

    # ìš”ê¸ˆì œì— ë”°ë¥¸ ì´ë¯¸ì§€ ì œí•œ (ì§€ê¸ˆì€ Proë§Œ ì“°ëŠ” êµ¬ì¡°ë¼ë©´ pro ê³ ì •)
    max_images = MAX_IMAGES_PRO if str(plan_type).lower().startswith("pro") else 20

    # 0) ì´ë¯¸ì§€ ì œí•œ
    if imgs and len(imgs) > max_images:
        st.warning(f"âš ï¸ ì´ë¯¸ì§€ê°€ ë§ì•„ ìƒìœ„ {max_images}ì¥ë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")
        imgs = imgs[:max_images]

    # 1) ë¹„ë””ì˜¤ ì²˜ë¦¬
    if video:
        if not HAS_MOVIEPY:
            st.error("ğŸš« ì„œë²„ì— 'moviepy'ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì˜ìƒ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            with st.spinner("ğŸ¬ ì˜ìƒ ì²˜ë¦¬ ì¤‘..."):
                audio_path = extract_audio_from_video(video)
                if audio_path:
                    text_chunks = process_audio_file(audio_path)
                    if text_chunks:
                        total_chunks = len(text_chunks)
                        for i, chunk_text in enumerate(text_chunks):
                            chunk_info = f"Segment {i+1}/{total_chunks}"
                            data = analyze_transcript_with_gpt(chunk_text, chunk_info).get("messages", [])
                            for item in data:
                                item = normalize_message_item(item)
                                item["source"] = "ì˜ìƒíŒŒì¼"
                                item["filename"] = video.name
                                if not item.get("context"):
                                    item["context"] = "ì˜ìƒ ë…¹ì·¨"
                            final_data.extend(data)

                    if os.path.exists(audio_path):
                        os.remove(audio_path)

    # 2) ì˜¤ë””ì˜¤ ì²˜ë¦¬
    if audio:
        if not HAS_PYDUB:
            st.error("ğŸš« ì„œë²„ì— 'pydub'ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì˜¤ë””ì˜¤ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            with st.spinner("ğŸ™ï¸ ë…¹ìŒ ë¶„ì„ ì¤‘..."):
                text_chunks = process_audio_file(audio)
                if text_chunks:
                    total_chunks = len(text_chunks)
                    for i, chunk_text in enumerate(text_chunks):
                        chunk_info = f"Part {i+1}/{total_chunks}"
                        data = analyze_transcript_with_gpt(chunk_text, chunk_info).get("messages", [])
                        for item in data:
                            item = normalize_message_item(item)
                            item["source"] = "ë…¹ìŒíŒŒì¼"
                            item["filename"] = audio.name
                            if not item.get("context"):
                                item["context"] = "í†µí™” ë…¹ìŒ"
                        final_data.extend(data)
                else:
                    st.warning(f"âš ï¸ ë…¹ìŒíŒŒì¼ '{audio.name}'ì—ì„œ ëŒ€í™”ë¥¼ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # 3) ì´ë¯¸ì§€ ì²˜ë¦¬ (ë³‘ë ¬)
    if imgs:
        batch_size = DEFAULT_BATCH_SIZE
        total_files = len(imgs)
        batch_indices = range(0, total_files, batch_size)
        total_batches = len(batch_indices)

        pbar = st.progress(0)
        status = st.empty()

        max_concurrent_workers = 3

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_workers) as executor:
            futures = set()
            file_pointer = 0

            while file_pointer < total_files or futures:
                while len(futures) < max_concurrent_workers and file_pointer < total_files:
                    current_batch_files = imgs[file_pointer:file_pointer + batch_size]
                    batch_data = []
                    for f in current_batch_files:
                        f.seek(0)
                        batch_data.append((f.name, f.read()))

                    batch_start_index = file_pointer
                    fut = executor.submit(analyze_image_batch_worker, batch_data, OPENAI_KEY, batch_start_index)
                    futures.add(fut)
                    file_pointer += batch_size

                if futures:
                    done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)
                    for fut in done:
                        futures.remove(fut)
                        try:
                            res_data, _ = fut.result()
                            final_data.extend(res_data)
                        except Exception as e:
                            print(f"Worker Exception: {e}")

                    processed_batches = (file_pointer // batch_size) - len(futures)
                    processed_batches = max(processed_batches, 0)
                    progress_val = min(processed_batches / max(total_batches, 1), 1.0)
                    pbar.progress(progress_val)
                    status.text(f"ğŸ“· ì´ë¯¸ì§€ ë¶„ì„ ì¤‘... ({processed_batches}/{total_batches} ë°°ì¹˜)")

        pbar.empty()
        status.empty()

    # 4) PDF ì²˜ë¦¬
    if pdf:
        if not HAS_PYPDF:
            st.error("ğŸš« ì„œë²„ì— 'pypdf'ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ PDF ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            with st.spinner("ğŸ“š PDF ë¶„ì„ ì¤‘..."):
                try:
                    pdf.seek(0)  # ì¤‘ìš”: í¬ì¸í„° ë³µêµ¬
                    reader = PdfReader(pdf)
                    full_text = ""
                    for page in reader.pages:
                        full_text += (page.extract_text() or "")

                    if not full_text.strip() or len(full_text.strip()) < 50:
                        st.warning(f"âš ï¸ ë¬¸ì„œ '{pdf.name}'ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê±°ì˜ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìŠ¤ìº”ë³¸(ì´ë¯¸ì§€)ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    else:
                        text_len = len(full_text)
                        chunk_size = MAX_PDF_CHUNK_SIZE
                        chunks = [full_text[i:i + chunk_size] for i in range(0, text_len, chunk_size)]
                        total_chunks = len(chunks)

                        progress_text = st.empty()
                        for i, chunk in enumerate(chunks):
                            progress_text.text(f"ğŸ“š PDF ë¶„ì„ ì¤‘... ({i+1}/{total_chunks} êµ¬ê°„)")
                            page_info = f"ì „ì²´ {total_chunks}êµ¬ê°„ ì¤‘ {i+1}ë²ˆì§¸ êµ¬ê°„"

                            data = analyze_pdf_chunk(chunk, page_info).get("messages", [])
                            for item in data:
                                item = normalize_message_item(item)
                                item["source"] = "PDFë¬¸ì„œ"
                                item["filename"] = pdf.name
                                if not item.get("context"):
                                    item["context"] = "ë¬¸ì„œ ë‚´ìš©"
                            final_data.extend(data)

                        progress_text.empty()

                except Exception as e:
                    st.error(f"PDF ì˜¤ë¥˜: {e}")

    return final_data


# =====================================
# 5) ì¦ê±° ZIP ë¡œì§
# =====================================
def get_image_taken_time(uploaded_file):
    try:
        uploaded_file.seek(0)
        with Image.open(uploaded_file) as img:
            exif_date = None
            exif = img.getexif()
            if exif:
                for tag_id in [36867, 36868, 306]:
                    val = exif.get(tag_id)
                    if val:
                        exif_date = val
                        break
            if exif_date:
                try:
                    return datetime.strptime(str(exif_date), "%Y:%m:%d %H:%M:%S")
                except Exception:
                    pass
    except Exception:
        pass
    finally:
        uploaded_file.seek(0)
    return None

def process_evidence_images_optimized(sorted_items):
    failed_files = []
    zip_data = None

    with tempfile.TemporaryDirectory() as src_dir:
        img_dir = os.path.join(src_dir, "images")
        os.makedirs(img_dir, exist_ok=True)

        html_lines = [
            "<!DOCTYPE html>",
            "<html><body><h1>Evidence Timeline</h1><hr/>"
        ]

        total = len(sorted_items)
        pbar = st.progress(0)
        status = st.empty()

        for idx, item in enumerate(sorted_items, 1):
            f = item["file"]
            ts = item["taken_at"]

            status.text(f"ğŸ“· ì²˜ë¦¬ ì¤‘... ({idx}/{total})")
            pbar.progress(idx / total)

            out_filename = f"{idx:04d}.jpg"
            out_path = os.path.join(img_dir, out_filename)

            try:
                f.seek(0)
                is_heic = f.name.lower().endswith((".heic", ".heif"))
                needs_conversion = is_heic

                if not is_heic:
                    try:
                        with Image.open(f) as img:
                            exif = img.getexif()
                            if exif and exif.get(274) and exif.get(274) > 1:
                                needs_conversion = True
                            if img.format not in ["JPEG", "JPG"]:
                                needs_conversion = True
                    except Exception:
                        needs_conversion = True

                f.seek(0)
                if not needs_conversion:
                    with open(out_path, "wb") as dest:
                        shutil.copyfileobj(f, dest)
                else:
                    with Image.open(f) as img:
                        img = ImageOps.exif_transpose(img)
                        if img.mode in ("RGBA", "P"):
                            img = img.convert("RGB")
                        img.save(out_path, "JPEG", quality=85)

                ts_str = ts.strftime("%Y-%m-%d %H:%M:%S") if ts else "íŒë…ë¶ˆê°€"
                html_lines.append(
                    f"<div><b>#{idx}</b> {ts_str}<br>"
                    f"<img src='images/{out_filename}' loading='lazy'/></div><hr/>"
                )

            except Exception as e:
                print(f"Evidence process fail: {f.name}, err={e}")
                failed_files.append(f.name)
                html_lines.append(
                    f"<div><b>#{idx}</b> [ì‹¤íŒ¨] {f.name}</div><hr/>"
                )

        html_lines.append("</body></html>")
        with open(os.path.join(src_dir, "timeline.html"), "w", encoding="utf-8") as f:
            f.write("\n".join(html_lines))

        status.text("ğŸ“¦ ZIP ì••ì¶• ì¤‘...")

        with tempfile.TemporaryDirectory() as out_dir:
            base_name = os.path.join(out_dir, "evidence_result")
            shutil.make_archive(base_name, "zip", src_dir)
            zip_path = base_name + ".zip"

            file_size_mb = os.path.getsize(zip_path) / (1024 * 1024)
            if file_size_mb > MAX_ZIP_SIZE_MB:
                st.error(
                    f"âŒ ìƒì„±ëœ ZIP íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤ ({file_size_mb:.1f}MB). "
                    f"ì„œë²„ ì•ˆì •ì„ ìœ„í•´ ë‹¤ìš´ë¡œë“œë¥¼ ì œí•œí•©ë‹ˆë‹¤. (ì œí•œ: {MAX_ZIP_SIZE_MB}MB)"
                )
                zip_data = None
            else:
                with open(zip_path, "rb") as f:
                    zip_data = f.read()

    pbar.empty()
    status.empty()
    return zip_data, failed_files

# =====================================
# 6) ë¡œê·¸ì¸ / íšŒì›ê°€ì… í˜ì´ì§€
# =====================================
def login_page():
    bg_color = "#0E1117" if st.session_state['is_dark_mode'] else "#f0f2f6"
    text_color = "#FAFAFA" if st.session_state['is_dark_mode'] else "#000000"
    
    st.markdown(f"""
    <style>
        [data-testid="stHeader"] {{ display: none; }}
        [data-testid="stToolbar"] {{ visibility: hidden; }}
        .stApp {{ background-color: {bg_color}; color: {text_color}; }}
        [data-testid="stSidebar"] {{ display: none !important; }}

        div.stButton > button {{
            width: 100%; height: 50px; font-size: 16px; font-weight: bold; border-radius: 8px;
        }}
        
        div[data-testid="stForm"] {{
            border: 1px solid #d1d5db;
            padding: 20px;
            border-radius: 10px;
            background-color: transparent; 
        }}

        div[data-baseweb="input"] {{
            background-color: #ffffff !important;
            border: 1px solid #d1d5db !important;
        }}
        input[type="text"], input[type="password"] {{
            background-color: #ffffff !important;
            color: #000000 !important;
            -webkit-text-fill-color: #000000 !important;
        }}
        label[data-testid="stLabel"] {{
            color: {text_color} !important;
            font-weight: bold !important;
        }}
    </style>
    """, unsafe_allow_html=True)

    top_col1, top_col2 = st.columns([8, 1])
    with top_col2:
        mode = st.toggle("ğŸŒ™", value=st.session_state['is_dark_mode'], key="login_toggle")
        if mode != st.session_state['is_dark_mode']:
            st.session_state['is_dark_mode'] = mode
            st.rerun()

    col1, col2, col3 = st.columns([1, 1.5, 1])
    with col2:
        st.markdown("<br><h1 style='text-align: center;'>âš–ï¸ Timeline.Ai</h1>", unsafe_allow_html=True)
        st.markdown(f"<p style='text-align: center; color: {text_color};'>ë²•ì  ì¦ê±° í†µí•© ë¶„ì„ ì‹œìŠ¤í…œ</p><hr>", unsafe_allow_html=True)

        if st.session_state["auth_mode"] == "login":
            st.subheader("ë¡œê·¸ì¸")
            with st.form("login_form"):
                email = st.text_input("ì´ë©”ì¼")
                pw = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password")
                st.markdown("<br>", unsafe_allow_html=True)
                login_submit = st.form_submit_button("ë¡œê·¸ì¸", type="primary", use_container_width=True)
            
            if login_submit:
                try:
                    res = supabase.auth.sign_in_with_password({"email": email, "password": pw})
                    if res.user:
                        st.session_state["user"] = {"id": res.user.id, "email": res.user.email}
                        st.success("ë¡œê·¸ì¸ ì„±ê³µ!")
                        st.rerun()
                except Exception as e:
                    st.error(f"ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
            
            st.write("") 
            if st.button("íšŒì›ê°€ì…", use_container_width=True):
                st.session_state["auth_mode"] = "signup"
                st.rerun()

        elif st.session_state["auth_mode"] == "signup":
            st.subheader("íšŒì›ê°€ì…")
            tab_email, tab_google = st.tabs(["ğŸ“§ ì´ë©”ì¼", "ğŸŒ Google"])
            
            with tab_email:
                with st.form("signup_form"):
                    new_email = st.text_input("ì´ë©”ì¼")
                    new_pw = st.text_input("ë¹„ë°€ë²ˆí˜¸ (6ì ì´ìƒ)", type="password")
                    new_pw2 = st.text_input("ë¹„ë°€ë²ˆí˜¸ í™•ì¸", type="password")
                    st.markdown("<br>", unsafe_allow_html=True)
                    signup_submit = st.form_submit_button("íšŒì›ê°€ì…", type="primary", use_container_width=True)

                if signup_submit:
                    if new_pw != new_pw2:
                        st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    elif len(new_pw) < 6:
                        st.error("ë¹„ë°€ë²ˆí˜¸ëŠ” 6ìë¦¬ ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
                    else:
                        try:
                            supabase.auth.sign_up({"email": new_email, "password": new_pw})
                            st.success("âœ… ê°€ì… ë©”ì¼ ì „ì†¡! ì´ë©”ì¼ ì¸ì¦ í›„ ë¡œê·¸ì¸í•˜ì„¸ìš”.")
                        except Exception as e:
                            st.error(f"ê°€ì… ì‹¤íŒ¨: {e}")

            with tab_google:
                st.info("Google ê³„ì •ìœ¼ë¡œ ê°„í¸ ê°€ì…/ë¡œê·¸ì¸")
                if st.button("Googleë¡œ ê³„ì†í•˜ê¸°", key="btn_google_join", use_container_width=True):
                    try:
                        res = supabase.auth.sign_in_with_oauth({
                            "provider": "google",
                            "options": {"redirect_to": REDIRECT_URL}
                        })
                        auth_url = getattr(res, "url", None) or getattr(res, "redirect_to", None)
                        if auth_url:
                            st.markdown(f'<a href="{auth_url}" target="_self">ğŸ‘‰ Google ë¡œê·¸ì¸ ì°½ ì—´ê¸°</a>', unsafe_allow_html=True)
                    except Exception as e:
                        st.error(f"Google ì¸ì¦ ì˜¤ë¥˜: {e}")

            st.markdown("---")
            if st.button("â¬…ï¸ ë’¤ë¡œê°€ê¸°", use_container_width=True):
                st.session_state["auth_mode"] = "login"
                st.rerun()

# =====================================
# 7) ë©”ì¸ ì•± í™”ë©´ (Nuclear Option: í† ê¸€ ê°•ì œ ì‚­ì œ + ìƒ‰ìƒ ê°•ì œ ì£¼ì…)
# =====================================
def main_app():
    # 1. ìƒ‰ìƒ ì •ì˜ (ë‹¤í¬ëª¨ë“œ/ë¼ì´íŠ¸ëª¨ë“œì— ë”°ë¥¸ í…ìŠ¤íŠ¸ ìƒ‰ìƒ ë³€ìˆ˜ ì„¤ì •)
    if st.session_state['is_dark_mode']:
        bg_color = "#0E1117"
        text_color = "#FAFAFA"  # ë‹¤í¬ëª¨ë“œì¼ ë•Œ ë©”ì¸ ê¸€ì”¨ëŠ” í°ìƒ‰
        sidebar_bg = "#262730"
    else:
        bg_color = "#FFFFFF"
        text_color = "#000000"  # ë¼ì´íŠ¸ëª¨ë“œì¼ ë•Œ ë©”ì¸ ê¸€ì”¨ëŠ” ê²€ì •
        sidebar_bg = "#F0F2F6"

    # 2. ê°•ë ¥í•œ CSS ìŠ¤íƒ€ì¼ ì£¼ì…
    st.markdown(f"""
    <style>
    /* [1] ì‚¬ì´ë“œë°” í† ê¸€ ë° í—¤ë” ìˆ¨ê¹€ */
    [data-testid="stSidebarCollapsedControl"],
    section[data-testid="stSidebar"] > div > div > button,
    [data-testid="stHeader"],
    [data-testid="stToolbar"],
    footer {{
        display: none !important;
    }}

    /* [2] ì•± ê¸°ë³¸ í…Œë§ˆ (ë°°ê²½ ë° ê¸°ë³¸ ê¸€ììƒ‰) */
    .stApp {{ background-color: {bg_color}; }}
    
    /* ê¸°ë³¸ í…ìŠ¤íŠ¸ë“¤ì€ ë³€ìˆ˜(text_color)ë¥¼ ë”°ë¼ê° -> ë‹¤í¬ëª¨ë“œë©´ í°ìƒ‰ */
    h1, h2, h3, h4, h5, h6, p, li, span, div {{ 
        color: {text_color}; 
    }}

    /* [3] ì‚¬ì´ë“œë°” ë°°ê²½ */
    [data-testid="stSidebar"] {{
        background-color: {sidebar_bg} !important;
    }}
    /* ì‚¬ì´ë“œë°” ê¸°ë³¸ í…ìŠ¤íŠ¸ë„ í…Œë§ˆ ìƒ‰ìƒ ë”°ë¼ê° */
    [data-testid="stSidebar"] p, 
    [data-testid="stSidebar"] span, 
    [data-testid="stSidebar"] div {{
        color: {text_color};
    }}

    /* [4] â˜…â˜…â˜… ì´ë©”ì¼ ë°•ìŠ¤ ì „ìš© (ì—¬ê¸°ëŠ” ë¬´ì¡°ê±´ ê²€ì •ìƒ‰ ê³ ì •) â˜…â˜…â˜… */
    .custom-email-box {{
        background-color: #FFFFFF !important; /* í°ìƒ‰ ë°°ê²½ */
        padding: 15px !important;
        border-radius: 10px !important;
        border: 1px solid #ddd !important;
        text-align: center !important;
        margin-bottom: 20px !important;
    }}
    /* ì´ë©”ì¼ ë°•ìŠ¤ ì•ˆì˜ ëª¨ë“  ìš”ì†ŒëŠ” ê°•ì œë¡œ ê²€ì •(#000000) */
    .custom-email-box * {{
        color: #000000 !important;
        -webkit-text-fill-color: #000000 !important;
        font-weight: bold !important;
    }}

    /* [5] â˜…â˜…â˜… íŒŒì¼ ì—…ë¡œë” ìŠ¤íƒ€ì¼ë§ ë¶„ë¦¬ â˜…â˜…â˜… */
    
    /* (A) ë¼ë²¨ (ex: 1. ìŠ¤í¬ë¦°ìƒ·, 2. ì¦ê±° ë¬¸ì„œ) -> í…Œë§ˆ ìƒ‰ìƒ(text_color) ë”°ë¦„ */
    label[data-testid="stWidgetLabel"],
    label[data-testid="stWidgetLabel"] p,
    label[data-testid="stWidgetLabel"] span {{
        color: {text_color} !important; /* ë‹¤í¬ëª¨ë“œ: í°ìƒ‰, ë¼ì´íŠ¸: ê²€ì • */
    }}

    /* (B) ë“œë¡­ì¡´ ë‚´ë¶€ (Drag and drop...) -> ë°°ê²½ì´ í°ìƒ‰ì´ë¯€ë¡œ ë¬´ì¡°ê±´ ê²€ì • */
    [data-testid="stFileUploaderDropzone"] {{
        background-color: #FFFFFF !important; 
    }}
    [data-testid="stFileUploaderDropzone"] small,
    [data-testid="stFileUploaderDropzone"] span,
    [data-testid="stFileUploaderDropzone"] div {{
        color: #000000 !important; /* ì—¬ê¸°ëŠ” ë¬´ì¡°ê±´ ê²€ì • */
        -webkit-text-fill-color: #000000 !important;
    }}
    
    /* (C) ì—…ë¡œë“œ ë²„íŠ¼ (Browse files) */
    [data-testid="stFileUploader"] button {{
        background-color: #FFFFFF !important; 
        color: #000000 !important;
        border: 1px solid #d1d5db !important;
    }}

    /* [6] ì‚¬ì´ë“œë°” ë²„íŠ¼ ë“± ê¸°íƒ€ */
    [data-testid="stSidebar"] .stButton button {{
        background-color: #FFFFFF !important;
        border: 1px solid #d1d5db !important;
    }}
    [data-testid="stSidebar"] .stButton button p {{
        color: #000000 !important; 
    }}
    </style>
    """, unsafe_allow_html=True)

    # --- ì‚¬ì´ë“œë°” ì˜ì—­ ---
    with st.sidebar:
        st.title("âš–ï¸ Timeline.Ai")
        st.markdown("---")
        
        user_email = st.session_state['user']['email'] if st.session_state['user'] else "GUEST"
        
        st.markdown(f"""
        <div class="custom-email-box">
            <span style="font-size: 20px;">ğŸ‘‹</span><br>
            <b style="font-size: 16px;">{user_email}</b>
            <span> ë‹˜</span>
        </div>
        """, unsafe_allow_html=True)

        st.write("âš™ï¸ **ì„¤ì •**")
        mode = st.toggle("ğŸŒ™ ë‹¤í¬ ëª¨ë“œ", value=st.session_state['is_dark_mode'], key="sidebar_dark_mode")
        if mode != st.session_state['is_dark_mode']:
            st.session_state['is_dark_mode'] = mode
            st.rerun()

        st.header("ìš”ê¸ˆì œ")
        st.info("Pro ìš”ê¸ˆì œ: ì›” 59,900ì›")
        plan_code = "pro"


        st.markdown("---")
        if st.button("ğŸ—‘ï¸ ë¶„ì„ ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.result_data = []
            st.rerun()

        if st.button("ë¡œê·¸ì•„ì›ƒ", use_container_width=True):
            supabase.auth.sign_out()
            st.session_state["user"] = None
            st.session_state["auth_mode"] = "login"
            st.rerun()

    # --- ë©”ì¸ ì»¨í…ì¸  ì˜ì—­ ---
    st.title("âš–ï¸ íƒ€ì„ë¼ì¸ ë³´ê³ ì„œ (Timeline.Ai)")
    st.subheader("ë²•ì  ì¦ê±° í†µí•© ë¶„ì„ ì‹œìŠ¤í…œ")

    with st.expander("â„¹ï¸ ì‚¬ìš© ê°€ì´ë“œ ë° ì£¼ì˜ì‚¬í•­ ë³´ê¸° (Click)"):
        st.write("""
                 **ê°œì¸ì •ë³´ë“¤ì€ ì ˆëŒ€ ë°ì´í„°ì— ë‚¨ê±°ë‚˜ í•™ìŠµë˜ì§€ ì•ŠìŠµë‹ˆë‹¤**
        1. **SNS ìº¡ì²˜**: ë‚ ì§œì™€ ì‹œê°„ì´ ì˜ ë³´ì´ê²Œ ì°ì–´ì£¼ì„¸ìš”.
        2. **ë…¹ìŒ íŒŒì¼**: MP3, M4A í˜•ì‹ì„ ì§€ì›í•˜ë©° 1ì‹œê°„ ì´ë‚´ íŒŒì¼ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        3. **ì£¼ì˜ì‚¬í•­**: ë³¸ ê²°ê³¼ë¬¼ì€ ë²•ì  íš¨ë ¥ì´ ì—†ëŠ” **'ì´ˆì•ˆ(Draft)'**ì…ë‹ˆë‹¤. ìµœì¢… ì œì¶œ ì „ ë°˜ë“œì‹œ ì›ë³¸ê³¼ ëŒ€ì¡°í•˜ì„¸ìš”.
        4. **SNS ì´ë¯¸ì§€**ëŠ” ì‹œê°„ìˆœì„œë³„ë¡œ ì˜¬ë ¤ì£¼ì‹œê¸¸ ë°”ëë‹ˆë‹¤. ê³¼ê±°>ìµœì‹ ìˆœì„œ.
        5. **ì¦ê±°ìë£Œê°€ ëŒ€ìš©ëŸ‰ì¸ ê²½ìš°ì—ëŠ” 2ë²ˆ~3ë²ˆ ë‚˜ëˆ„ì–´ ì˜¬ë ¤ì£¼ì‹œê¸¸ ë°”ëë‹ˆë‹¤.** ì˜ˆ) ì²«ë²ˆì§¸:ì´ë¯¸ì§€+PDF ë‘ë²ˆì§¸:ë…¹ìŒíŒŒì¼
        6. **PDFíŒŒì¼**ì— ì´ë¯¸ì§€ê°€ ë‹´ê²¨ìˆëŠ” ê²½ìš°, ì´ë¯¸ì§€ëŠ” ë¶„ì„ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ëŠ” SNSí…ìŠ¤íŠ¸ ë¶„ì„ì´ë‚˜, ì¦ê±°ì´ë¯¸ì§€ì¸ ê²½ìš°ì—” ZIPì¹¸ì— ì˜¬ë ¤ì£¼ì„¸ìš”
        """)

    tab1, tab2, tab3 = st.tabs(["ğŸ“‚ 1. ì¦ê±° ì—…ë¡œë“œ", "ğŸ“Š 2. ë¶„ì„ ê²°ê³¼ í™•ì¸", "ğŸ§¾ 3. ì¦ê±°ì´ë¯¸ì§€ íƒ€ì„ë¼ì¸ ZIP ìƒì„±"])

    with tab1:
        imgs_in, audio_in, video_in, pdf_in = None, None, None, None

        st.success("ğŸ’ **Pro**: ëª¨ë“  ê¸°ëŠ¥ ë¬´ì œí•œ (ë…¹ìŒ/ì˜ìƒ í¬í•¨)")

        c1, c2 = st.columns(2)
        with c1:
            st.markdown("### ğŸ“· ì´ë¯¸ì§€ / ğŸ¤ ë…¹ìŒ")
            imgs_in = st.file_uploader("1. SNS ìŠ¤í¬ë¦°ìƒ·", type=['png', 'jpg', 'jpeg', 'heic'], accept_multiple_files=True, key="p_img")
            audio_in = st.file_uploader("3. ë…¹ìŒ íŒŒì¼", type=['mp3', 'm4a', 'wav'], key="p_audio")

        with c2:
            st.markdown("### ğŸ“„ ë¬¸ì„œ / ğŸ¬ ì˜ìƒ")
            pdf_in = st.file_uploader("2. ì¦ê±° ë¬¸ì„œ", type=['pdf'], key="p_pdf")
            video_in = st.file_uploader("4. ì˜ìƒ íŒŒì¼", type=['mp4', 'avi'], key="p_video")

        st.write("")
        if st.button("í†µí•© ë¶„ì„ ì‹œì‘ ğŸš€", type="primary"):
            if not any([imgs_in, audio_in, video_in, pdf_in]):
                st.warning("íŒŒì¼ì„ í•˜ë‚˜ë¼ë„ ì˜¬ë ¤ì£¼ì„¸ìš”.")
            else:
                res = run_analysis(imgs_in, audio_in, video_in, pdf_in, plan_code)
                st.session_state.result_data = res
                if res:
                    st.toast('ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íƒ­ì„ í™•ì¸í•˜ì„¸ìš”.', icon='âœ…')
                else:
                    st.toast('ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.', icon='âš ï¸')


    with tab2:
        if 'result_data' in st.session_state and st.session_state.result_data:
            df = pd.DataFrame(st.session_state.result_data)
            if 'ID' not in df.columns: df.insert(0, 'ID', range(1, 1 + len(df)))

            # =========================================================
            # [ìˆ˜ì • 3/3] timestamp ê¸°ë°˜ ì •ë ¬ ì œê±°
            # - ëŒ€ì‹  upload_indexê°€ ìˆìœ¼ë©´ ì—…ë¡œë“œ ìˆœì„œë¡œ ì •ë ¬
            # =========================================================
            if 'upload_index' in df.columns:
                df = df.sort_values(by='upload_index', na_position='last')
                df = df.drop(columns=['upload_index'])

            req_cols = ['ID', 'timestamp', 'date', 'time', 'context', 'sender', 'content', 'importance', 'source', 'link', 'filename', 'is_estimated']
            for c in req_cols:
                if c not in df.columns: df[c] = ""
            df = df[req_cols]

            st.subheader("ğŸ“Š ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸")
            sub1, sub2 = st.tabs(["ğŸ“‹ ê²°ê³¼ ì—‘ì…€", "ğŸ’¯ ì •í™•ë„ ê²€ì¦"])
            
            with sub1:
                st.success("âœ… ë¶„ì„ ì™„ë£Œ! ì•„ë˜ ë‚´ìš©ì„ í™•ì¸í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”.")
                edited_df = st.data_editor(
                    df,
                    num_rows="dynamic", use_container_width=True,
                    column_config={
                        "is_estimated": st.column_config.CheckboxColumn("ë‚ ì§œì¶”ì •"),
                        "importance": st.column_config.SelectboxColumn("ì¤‘ìš”ë„", options=["ìƒ", "ì¤‘", "í•˜", "ë¯¸ìƒ"]),
                    },
                )

                if HAS_OPENPYXL:
                    export_buffer = io.BytesIO()
                    with pd.ExcelWriter(export_buffer, engine='openpyxl') as writer:
                        edited_df.to_excel(writer, sheet_name='ì „ì²´ íƒ€ì„ë¼ì¸', index=False)
                        if 'source' in edited_df.columns:
                            unique_sources = edited_df['source'].unique()
                            for src in unique_sources:
                                if pd.isna(src) or src == "": safe_name = "ê¸°íƒ€"
                                else: safe_name = str(src).replace("/", "_").replace("\\", "_")[:30]
                                subset_df = edited_df[edited_df['source'] == src]
                                if not subset_df.empty:
                                    subset_df.to_excel(writer, sheet_name=safe_name, index=False)
                    
                    st.download_button(
                        label="ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (ì‹œíŠ¸ ë¶„ë¦¬ë¨)",
                        data=export_buffer.getvalue(),
                        file_name="ì¦ê±°_íƒ€ì„ë¼ì¸_ë¶„ì„.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        use_container_width=True
                    )
                else:
                    st.warning("âš ï¸ ì—‘ì…€ ì €ì¥ ë¼ì´ë¸ŒëŸ¬ë¦¬(openpyxl)ê°€ ì—†ì–´ CSVë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
                    st.download_button(
                        label="ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                        data=edited_df.to_csv(index=False).encode('utf-8-sig'),
                        file_name="ì¦ê±°_íƒ€ì„ë¼ì¸_ë¶„ì„.csv",
                        mime="text/csv",
                        use_container_width=True
                    )

            with sub2:
                st.info("ğŸ§ **ì •ë‹µì§€(Ground Truth)ë¥¼ ì—…ë¡œë“œí•˜ë©´ AI ì ìˆ˜ë¥¼ ì¦‰ì‹œ ê³„ì‚°í•©ë‹ˆë‹¤.**")
                upl_truth = st.file_uploader("ğŸ“‚ ì •ë‹µ ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ", type=['xlsx'], key="truth_up")
                if upl_truth:
                    try:
                        df_truth = pd.read_excel(upl_truth)
                        score, rpt_df = evaluate_results(df_truth, df)
                        st.metric(label="ğŸ† AI ì¢…í•© ì •í™•ë„", value=f"{score:.1f}ì ")
                        st.dataframe(rpt_df, use_container_width=True)
                    except Exception as e: st.error(f"ì •ë‹µ íŒŒì¼ ì˜¤ë¥˜: {e}")
        else:
            st.info("ğŸ‘ˆ 'ì¦ê±° ì—…ë¡œë“œ' íƒ­ì—ì„œ ë¶„ì„ì„ ì‹œì‘í•˜ë©´ ì—¬ê¸°ì— ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤.")

    with tab3:
        st.subheader("ğŸ§¾ ì¦ê±° ì´ë¯¸ì§€ ZIP ìƒì„±")
        st.info("ì´ë¯¸ì§€ë“¤ì˜ EXIF ì •ë³´(ì´¬ì˜ì¼)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìë™ ì •ë ¬í•˜ê³ , HTML ë¦¬í¬íŠ¸ì™€ í•¨ê»˜ ì••ì¶•í•©ë‹ˆë‹¤.")
        
        e_imgs = st.file_uploader(
            "ì¦ê±°ìš© ì›ë³¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ (JPG, PNG, HEIC)",
            accept_multiple_files=True,
            type=["jpg", "png", "heic", "jpeg"],
            key="evi_zip"
        )
        
        if st.button("ZIP ìƒì„± ì‹œì‘", type="primary", key="btn_zip"):
            if not e_imgs:
                st.warning("íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            else:
                items = []
                for f in e_imgs:
                    items.append({
                        "file": f,
                        "taken_at": get_image_taken_time(f)
                    })

                items.sort(
                    key=lambda x: (x["taken_at"] if x["taken_at"] else datetime.max)
                )

                zip_bytes, fails = process_evidence_images_optimized(items)
                if zip_bytes:
                    st.success("âœ… ZIP íŒŒì¼ ìƒì„± ì™„ë£Œ!")
                    st.download_button(
                        "ğŸ“¥ Evidence.zip ë‹¤ìš´ë¡œë“œ",
                        zip_bytes,
                        "evidence.zip",
                        "application/zip",
                        use_container_width=True
                    )
                if fails:
                    st.error(f"âš ï¸ {len(fails)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ (ì†ìƒë˜ì—ˆê±°ë‚˜ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹)")

# =====================================
# 8) ì‹¤í–‰ íë¦„ ì œì–´ ë° ì„¸ì…˜ ë³µêµ¬ (Auth Check)
# =====================================
if st.session_state["user"] is None:
    try:
        session = supabase.auth.get_session()
        if session and session.user:
            st.session_state["user"] = {"id": session.user.id, "email": session.user.email}
    except Exception:
        pass

if st.session_state["user"] is None:
    login_page()
else:
    main_app()
